# Amazon S3 - Basic

- 많은 웹은 Amazon S3에 의존
- 예를 들어, 많은 웹사이트에서는 Amazon S3를 중추로 활용하며, 많은 AWS 서비스는 또한 Amazon S3를 통합을 위해 사용하고 있음
- 본질적으로 S3는 스토리지

## Amazon S3 Use Cases 

- Amazon S3는 백업과 스토리지로 활용됨
	- 파일용일 수도 있고 디스크용일 수도 있음
- 재해 복구의 용도로도 쓰임
	- 예를 들어 리전이 다운되는 경우 여러분은 데이터를 다른 리전으로 이동시킴
	- 그러면 해당 데이터는 어딘가에 백업됨
- 아카이브용으로도 사용
	- Amazon S3에 파일을 아카이브해 두면 추후 매우 손쉽게 검색할 수 있음
- 다음 용도는 하이브리드 클라우드 스토리
	- 온프레미스에 스토리지가 있지만 클라우드로 확장하고자 한다면 Amazon S3를 사용할 수 있음
- 애플리케이션을 호스팅하고 동영상 파일이나 이미지 등 미디어를 호스팅할 수 있음
- 데이터 레이크를 보유하여 다량의 데이터를 저장하고 빅 데이터 분석을 수행하기 위해서도 사용됨
- 정적 웹 사이트를 호스팅하기 위해서 등 다양한 용도로 쓰임
- 2가지 사용 사례
	- Nasdaq은 7년간의 데이터를 S3 Glacier에 저장해 둠
		- 이는 Aamzon S3의 아카이브 서비스와 유사
	- Sysco는 자체 데이터에 관한 분석을 실행하여 Amazon S3로부터 비즈니스 인사이트를 얻음

## Amazon S3 Buckets

- Amazon S3는 파일을 버킷에 저장하는데, 버킷은 상위 레벨 디렉토리로 표시됨
- S3 버킷의 파일은 객체라고 하며 이 버킷은 계정 안에 생성됨
- 버킷에는 전역적으로 고유한 이름이 있어야 함
	- 즉, 이름은 계정에 있는 모든 리전과 AWS에 존재하는 모든 계정에서 고유해야 함
	- AWS에서 전역적으로 고유한 단 하나의 이름
- 버킷은 리전 수준에서 정의됨
	- 버킷 이름이 모든 리전과 모든 계정에서 고유해도 버킷은 반드시 특정 AWS 리전에서 정의되어야 함
	- S3는 전역 서비스처럼 보이지만 버킷은 사실상 리전에서 생성됨
- S3 버킷에는 명명 규칙이 있음
	- 버킷 이름에는 대문자나 밑줄이 없어야 함
	- 길이는 3자에서 63자 사이여야 하며 IP여서는 안 되고 소문자나 숫자로 시작해야 함
	- 몇 가지 접두사 제한도 있음
	- 문자와 숫자, 하이픈만 사용하면 됨

## Amazon S3 Objects
### Key

- 객체나 파일에는 키라는 것이 있는데, Amazon S3 키는 파일의 전체 경로
- 예시
	- my_file.txt의 키: my_folder1/another_folder/my_file.txt
	- 이렇게 키는 접두사와 객체 이름으로 구성
	- 이전의 경로를 my_folder1와 another_folder라는 접두사, 그리고 my_file.txt라는 객체 이름으로 분해 가능
- Amazon S3 그 자체로는 디렉토리의 개념은 없고, 핵심은 키
### Object

- 객체값은 본문의 내용
- 파일 등 원하는 것은 뭐든지 Amazon S3로 업로드할 수 있음
- 최대 객체 크기는 5TB
- 한 번의 업로드 작업은 5GB를 초과할 수 없음
- 파일이 5GB보다 크다면 멀티파트 업로드를 사용해서 해당 파일을 여러 부분으로 나눠 업로드해야 함
	- 따라서 5TB의 파일을 가지고 있다면 최소 1000개 부분으로 나눠서 업로드해야 함
- Metadata
	- 객체의 키-값 쌍 리스트
	- 시스템이나 사용자에 의해 설정되어 파일에 관한 요소나 메타데이터를 나타낼 수 있음
- Tags
	- 태그의 경우 가령 유니코드 키-값 쌍은 최대 10개까지 가능
	- 태그는 보안과 수명 주기에 유용
- Version ID (If versioning is enabled)
	- 때때로 객체는 버전 ID를 갖기도 함
	- 버전 관리를 활성화한 경우

## Amazon S3 - Security

-  User-Based Security
	- 사용자에게는 IAM 정책이 주어지는데, 이 정책은 어떤 API 호출이 특정 IAM 사용자를 위해 허용되어야 하는지를 승인함
- Resource-Based Security
	- 새로운 보안 기능
	- S3 버킷 정책
		- S3 버킷의 보안을 관리하는 가장 일반적인 방법
		- 이 정책은 S3 콘솔에서 직접 할당할 수 있는 전체 버킷 규칙
		- 예를 들어, 특정 사용자가 들어올 수 있게 하거나 다른 계정의 사용자를 허용할 수 있음
			- 이를 S3 버킷에 액세스할 수 있는 교차 계정이라고 함
		- 이는 S3 버킷을 공개로 만드는 방법이기도 함
	- Object Access Control List (ACL) 
		- 이 목록은 보다 세밀한 보안이며 비활성화할 수 있음
	- Bucket Access Control List
		- 버킷 수준의 버킷 ACL이 있는데, 훨씬 덜 일반적임
		- 이 역시 비활성화 가능
- Encryption
	- Amazon S3 보안을 관리할 수 있는 또 다른 방법은 암호 키를 사용하여 객체를 암호화하는 것
	- 
- IAM 원칙이 S3 객체에 액세스할 수 있는 상황
	- IAM 권한이 이를 허용하거나 리소스 정책이 이를 허용하는 경우
		- IAM 원칙이 특정 API 호출 시 S3 객체에 액세스할 수 있음

### Amazon S3 Bucket Policies

![as1](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as1.png)

- S3 보안의 주안점은 버킷 정책
- JSON 기반의 정책
	- Resource
		- 리소스는 이 정책이 적용되는 버킷과 객체를 정책에 알려줌
		- 위 예시에서는 examplebucket 내에서 모든 객체에 적용되는 것을 볼 수 있음
			- 별표가 쓰인 이유
	- Effect
		- 허용 아니면 거부
		- Action을 허용하거나 거부하는 것
	- Action
		- 허용하거나 거부할 수 있는 API 집합
		- 예시에서 허용한 작업은 GetObject
	- Principle
		- 원칙은 정책을 적용할 계정 또는 사용자를 제시
		- 예시에서 원칙은 별표로 표시되었는데 이 경우 별표를 통해 모든 사용자를 허용할 수 있음
			- 따라서 이 S3 버킷은 버킷 내 모든 객체에 대해 공개 읽기로 설정하고 있음
- 필요한 버킷 정책을 사용해서 버킷에 대한 공개 액세스를 허용할 수 있으며  업로드 시 객체를 강제로 암호화하거나 또 다른 계정으로의 액세스를 허용할 수도 있음

#### Example: Public Access - Use Bucket Policy

![as2](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as2.png)

- 공개 액세스를 위한 버킷 정책이 있다고 가정
- 사용자는 월드 와이드 웹에 있는데, 이 웹사이트 방문자는 S3 버킷 안에 있는 파일에 액세스하려고 함
- 필요한 버킷 정책을 첨부해서 공개 액세스를 허용할 것임
- 버킷 정책이 S3 버킷에 첨부되면 그 안에 있는 모든 객체에 액세스할 수 있음

#### Example: User Access to S3 - Use IAM Policy

![as3](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as3.png)

- 계정 내 사용자, 즉 IAM 사용자가 있는 경우 해당 사용자가 Amazon S3에 액세스하고자 함
- 정책을 통해 이 사용자에게 IAM 권한을 할당할 수 있음
- 정책이 S3 버킷으로의 액세스를 허용하므로 해당 사용자는 S3 버킷에 액세스할 수 있음

#### Example: EC2 Instances Access - Use IAM Roles

![as4](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as4.png)

- EC2 인스턴스가 있는 경우, EC2 인스턴스에서 S3 버킷으로 액세스를 제공할 수 있음
- IAM 역할을 사용해야 함
- 올바른 IAM 권한을 통해 EC2 인스턴스 역할을 생성
- 그러면 IAM 역할에 해당되는 EC2 인스턴스는 Amazon S3 버킷에 액세스할 수 있음

#### Advanced: Cross-Account Access – Use Bucket Policy

![as5](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as5.png)

- 교차 계정 액세스를 허용하려면 버킷 정책을 사용해야 함
- 다른 계정에 IAM 사용자가 있을 때, 추가 버킷 정책을 생성해서 이 특정 IAM 사용자에게 교차 계정 액세스를 허용
- 따라서 IAM 사용자는 S3 버킷으로 API 호출을 할 수 있게 됨

### Bucket Setting for Block Public Access

![as6](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as6.png)

- 블록 공개 액세스에 관한 버킷 설정
- 버킷을 생성할 때 설정
- 이 설정은 기업 데이터 유출을 방지하기 위한 추가 보안 계층으로서 AWS가 개발함
- S3 버킷 정책을 설정하여 공개로 만들더라도 이 설정이 활성화되어 있다면 버킷은 결코 공개되지 않음
- 버킷을 공개하면 안 되는 경우, 이 설정을 그대로 두면 잘못된 S3 버킷 정책을 설정한 사람들에 대해 이러한 수준의 보안을 갖출 수 있음
- S3 버킷 중 어느 것도 공개되서는 안 된다면 계정 수준에서 이를 설정하면 됨

## Amazon S3 - Static Website Hosting

![as7](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as7.png)

- S3는 웹 사이트를 호스팅하고 인터넷에서 액세스할 수 있게 만들 수 있음
- 웹 사이트 URL은 이것을 생성하는 AWS 리전에 따라 달라지는데, 위 예시에 나와있는 것 둘 중 하나가 됨
	- 유일한 차이점으로는 이곳의 대시와 점
- 예시
	- 버킷에는 HTML 파일이나 이미지 파일 등이 있다고 가정
	- 해당 URL과 함께 이렇게 나타나는데, 그럼 사용자는 S3 버킷에 액세스할 수 있게됨
	- 하지만 버킷에서 공개 읽기가 활성화되지 않은 경우 이것은 작동하지 않을 것임
		- 읽기를 위해 버킷을 활성화한 후 403 Forbidden 오류가 발생했다면 버킷이 공개가 아니라는 뜻
		  - 따라서 공개를 허용하는 S3 버킷 정책을 첨부해야 함

## Amazon S3 - Versioning

![as8](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as8.png)

- 안전한 방법으로 생성한 웹 사이트를 업데이트할 방법으로, Amazon S3에는 파일 버전 관리가 있음
- 버킷 수준에서 활성화해야 하는 설정
- 예시
	- 버킷이 주어졌고, 버전 관리로 활성화된 상태라고 가정
	- 사용자가 파일을 업로드할 때마다 선택 키에서 해당 파일의 버전이 생성됨
	- 동일한 키를 업로드하고 해당 파일을 덮어쓰는 경우 버전 2, 버전 3 등을 생성
	- 의도하지 않게 삭제하지 않도록 보호해줌
		- 따라서 버킷을 버전 관리하는 것이 좋음
		- 한 파일 버전을 삭제하는 경우 사실상 삭제 마커를 추가한 것
			- 이전 버전을 복구할 수 있다는 것
			- 이전 버전으로 쉽게 롤백할 수도 있음
		- 2일 전 상태가 어땠는지 확인하고 싶다면 파일을 취해서 롤백하면 됨
- 유의해야 할 사항
	- 버전 관리를 활성화하기 전에 버전 관리가 적용되지 않은 모든 파일은 널(null) 버전을 갖게됨
	- 버전 관리를 중단해도 이전 버전을 삭제하지는 않음

## Amazon S3 - Replication (CRR & SRR)

![as9](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as9.png)

- 아마존 S3의 복제 기능
	- 어떤 특정한 리전에 S3 버킷이 있고, 이를 다른 혹은 같은 리전의 S3 버킷에 복제해야 할 때 사용
- **소스 버킷과 복제 대상 버킷 둘 모두 버전 관리 기능이 활성화되어야 함**
- 복제를 활성화한 후에는 새로운 객체만 복제 대상이 됨
	- 기존의 객체를 복제하려면 S3 배치 복제 기능을 사용해야 함
	- 기존 객체부터 복제에 실패한 객체까지 복제할 수 있는 기능
- 작업을 삭제하려면 소스 버킷에서 대상 버킷으로 삭제 마커를 복제하면 됨
	- 설정에서 선택할 수 있음
	- 버전 ID로 삭제하는 경우 버전 ID는 복제되지 않음
		- 영구적인 삭제로 누군가 악의를 품고 한 버킷에서 다른 버킷으로 ID 삭제 마커를 복제하면 안 되기 때문
- 체이닝 복제는 불가
	- 1번 버킷이 2번 버킷에 복제되어 있고 2번 버킷이 3번 버킷에 복제돼 있다고 해서 1번 버킷의 객체가 3번 버킷으로 복제되지 않음
- Cross-Region Replication (CRR)
	- 복제가 일어나는 두 S3의 리전이 달라야 함
- Same-Region Replication (SRR)
	- 복제가 일어나는 두 S3의 리전이 같아야 함
- 버킷 간에는 비동기식 복제가 일어남
- 버킷은 서로 다른 AWS 계정 간에도 사용 가능
- 복제 과정은 백그라운드에서 이루어짐
- 복제 기능이 정상적으로 실행되려면, S3에 올바른 IAM 권한, 즉 읽기, 쓰기 권한을 S3에 부여해야 함
- Use Cases
	- CRR
		- 컴플라이언스, 즉 법규나 내부 체제 관리, 그리고 데이터가 다른 리전에 있어 발생할 수 있는 지연 시간을 줄일 경우에 사용
		- 계정간 복제에도 쓸 수 있음
	- SRR
		- 다수의 S3 버킷간의 로그를 통합할 때
		- 개발 환경이 별도로 있어 운영 환경과 개발 환경간의 실시간 복제를 필요로 할 때

## S3 Storage Class

- Amazon S3 Standard: General Purpose
- Amazon S3 Standard-Infrequent Access (IA)
- Amazon S3 One Zone-Infrequent Access
- Amazon S3 Glacier Instant Retrieval
- Amazon S3 Glacier Flexible Retrieval
- Amazon S3 Glacier Deep Archive
- Amazon S3 Intelligent Tiering
- Amazon S3에서 객체를 생성할 때 클래스를 선택할 수도 있고 스토리지 클래스를 수동으로 수정할 수도 있음
- 혹은 Amazon S3 수명 주기 구성을 사용해 스토리지 클래스 간에 객체를 자동으로 이동할 수도 있음

### S3 Durability and Availability

- 내구성 (Durability)
	- Amazon S3로 인해 객체가 손실되는 횟수
	- Amazon S3는 매우 뛰어난 내구성을 제공
	- 9가 11개 즉, 99.999999999%의 내구성을 보장 (11 9s)
	- Amazon S3에 천만 개의 객체를 저장했을 때 평균적으로 10,000년에 한 번 객체 손실이 예상
	- 모든 스토리지 클래스의 내구성은 동일
- 가용성 (Availability)
	- 서비스가 얼마나 용이하게 제공되는지를 나타냄
	- 스토리지 클래스에 따라 다름
	- 예를 들어 S3 Standard의 가용성은 99.99%
		- 즉, 1년에 약 53분 동안은 서비스를 사용할 수 없다는 의미
	- 다시 말해 서비스를 사용할 때 몇 가지 에러가 발생한다는 뜻
	- 애플리케이션을 개발할 때 이 부분을 고려해 두어야 함

### S3 Standard - General Purpose

- S3 Standard의 가용성은 99.99%
- 자주 액세스하는 데이터에 사용
- 기본적으로 사용하는 스토리지 유형
- 지연 시간이 짧고 처리량이 높으며 AWS에서 두 개의 기능 장애를 동시에 버틸 수 있음
- Use Cases
	- 빅 데이터 분석
	- 모바일과 게임 애플리케이션
	- 콘텐츠 배포

### S3 Infrequent Access

- S3 Infrequent Access는 자주 액세스하지는 않지만 필요한 경우 빠르게 액세스해야 하는 데이터를 저장
- S3 Standard보다 비용이 적게 들지만 검색 비용이 발생
- S3 Standard-IA의 가용성은 99.9%로 S3 Standard에 비해 약간 떨어짐
- Use Cases
	- 재해 복구와 백업

### S3 One Zone Infrequent Access

- Amazon S3 One Zone Infrequent Access는 One Zone-IA로도 불림
- 단일 AZ 내에서는 높은 내구성을 갖지만 AZ가 파괴된 경우 데이터를 잃게 됨
- 가용성은 더 낮은 수준인 99.5%
- Use Cases
	- 온프레미스 데이터를 2차 백업
	- 재생성 가능한 데이터를 저장

### S3 Glacier Storage

- Glacier Storage는 Glacier는 이름에서 알 수 있듯이 콜드 스토리지
- 아카이빙과 백업을 위한 저비용 객체 스토리지
- 스토리지 비용과 검색 비용이 발생

#### Glacier 스토리지의 세 가지 클래스

- **S3 Glacier Instant Retrieval**
	- 밀리초 단위로 검색이 가능
	- 예를 들어 분기에 한 번 액세스하는 데이터에 아주 적합
	- Instant는 즉시 처리된다는 의미
	- 최소 보관 기간이 90일이기 때문에 백업이지만 밀리초 이내에 액세스해야 하는 경우 적합
- **S3 Glacier Flexible Retrieval**
	- Amazon Glacier Flexible Retrieval에는 세 가지 옵션이 있음
		- Expedited: 데이터를 1~5분 이내에 받을 수 있음
		- Standard: 데이터를 돌려받는 데 3~5시간 소요
		- Bulk: 무료지만 데이터를 돌려받는 데 5~12시간이 소요
	- Flexible는 데이터를 검색하는 데 최대 12시간까지 기다려야 한다는 의미
	- 최소 보관 기간은 역시 90일
- **S3 Glacier Deep Archive**
	- 장기 보관 목적
	- 두 가지 검색 티어
		- Standard: 12시간
		- Bulk: 48시간
	- 데이터를 검색하는데 오래 걸리긴 하지만 비용이 가장 저렴
	- 최소 보관 기간은 180일

### S3 Intelligent Tiering

- 사용 패턴에 따라 액세스된 티어 간에 객체를 이동할 수 있게 해줌
- 소액의 월별 모니터링 비용과 티어링 비용이 발생하지만 검색 비용이 없음
- Tier
	- Frequent Access: 자동이고 기본 티어
	- Infrequent Access: 30일 동안 액세스하지 않는 객체 전용 티어
	- Archive Instant Access: 자동이지만 90일 동안 액세스하지 않는 티어
	- Archive Access: 선택 사항이며 90일에서 700일 이상까지 구성할 수 있고 
	- Deep Archive Access: 180일에서 700일 이상 액세스하지 않는 객체에 구성 가능
- S3 Intelligent Tiering은 알아서 객체를 이동시켜 주기 때문에 편하게 스토리지 관리 가능

### 모든 스토리지 클래스를 비교하는 표

![as10](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as10.png)

### us-east-1에 대한 예시 가격표

![as11](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as11.png)

# Amazon S3 - Advanced
## Amazon S3 - Moving between Storage Classes

![as12](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as12.png)

- 스토리지 클래스 간 데이터 이동
	- 스토리지 클래스 간에 객체를 이동시킬 수 있음
- 자신보다 아래에 있는 모든 수준으로 이동 가능
	- Standard에서는 Standard IA나 Intelligent Tiering으로 이동 가능
	- One-Zone IA에서는 Glacier Instant Retrieval Flexible과 Deep Archive로 이동 가능
	- 물론 Standard IA에서 바로 Glacier Flexible Retrieval로 갈 수도 있음
	- 객체에 자주 액세스하지 않으면 Standard IA로 옮기고 객체를 아카이브하려면 Glacier Deep Archive 티어로 옮기는 것을 추천
- 객체는 수동으로 옮기거나 수명 주기 규칙을 이용해서 자동화할 수 있음

## Amazon S3 - Lifecycle Rules

- 전환 작업 (Transaction Actions)
	- 다른 스토리지 클래스로 객체를 전환하도록 구성
	- 생성 60일 후에 Standard IA로 이동하도록 설정하거나 6개월 후에 Glacier에 아카이빙되도록 설정할 수 있음
- 만료 작업 (Exipration Actions)
	- 만료 작업도 설정할 수 있어 일정 시간이 지나면 객체가 삭제 또는 만료되게 할 수 있음
	- 365일 후에 액세스 로그 파일을 삭제하도록 하거나 버저닝을 활성화한 경우 이전 버전의 파일을 삭제하도록 설정 가능
	- 혹은 완료되지 않은 멀티파트 업로드는 삭제하도록 구성 가능
		- 지금쯤 업로드가 끝났어야 하나 2주째 업로드되지 않고 있는 멀티파트 업로드는 삭제하도록 설정할 수 있음
- 특정 접두사를 사용하여 전체 버킷이나 버킷의 일부 경로에만 적용할 수 있고 특정 객체 태그에만 지정할 수도 있음
	- 가령 '회계과' 태그가 붙은 객체에만 규칙을 적용 가능

### Lifecycle Rules - Scenario

- Scenario 1
	- EC2에 애플리케이션이 있고 Amazon S3에 업로드되는 프로필 사진의 이미지 섬네일을 생성하려 함
	- 썸네일은 원본 사진에서 재생성하기 쉬우므로 60일 동안만 보관
	- 소스 이미지는 60일 동안은 바로 검색할 수 있어야 하고 이후에는 사용자가 6시간까지 기다릴 수 있음
- 설계 1
	- S3 소스 이미지를 Standard 클래스에 두고 수명 주기 구성을 한 다음 60일 이후에 Glacier로 전환
	- 썸네일 이미지를 지정하려면 접두사를 이용해서 섬네일과 소스 이미지를 구분할 수 있음
	- 섬네일은 One-Zone IA에 둘 수 있는데 이는 해당 이미지 액세스가 잦지 않고 재생성하기 쉽기 때문
	- 그리고 수명 주기 구성을 통해 60일 이후에 만료 또는 삭제
- Scenario 2
	- 회사에 다음과 같은 규칙이 있음
		- S3 객체 삭제 후 30일 이내에는 즉시 복구가 가능해야 하고 그로부터 365일 이내에는 해당 객체를 48시간 내에 복구할 수 있어야 함
- 설계 2
	- S3 버저닝을 활성화하여 객체 버전을 유지 및 보유하고 있어야 삭제 마커 뒤에 숨어 있는 삭제된 객체를 복구할 수 있음
	- 그리고 객체의 이전 버전을 Standard AI로 이동시키는 전환 규칙을 생성해야 함
		- 최상위 버전이 아니라는 뜻
	- 그런 다음 이전 버전을 Glacier Deep Archive로 보내 아카이브하는 규칙이 있어야 함

## Amazon S3 Analytics - Storage Class Analysis

![as13](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as13.png)

- 한 클래스에서 다른 클래스로 객체를 전환하는 데에 있어 최적의 기간을 결정할 수 있음
- Standard와 Standard IA용 권장 사항을 제공하고 One-Zone IA나 Glacier은 해당되지 않음
- S3 버킷에서 S3 Analytics를 실행하면 버킷에 관한 권장 사항과 통계를 csv 파일로 제공
- 이 보고서는 매일 업데이트되고 해당 데이터 분석 결과까지 24시간에서 48시간이 소요
- 이 csv 보고서로 수명 주기 규칙을 알아보고 개선 방향을 생각해 볼 수 있음

## Amazon S3 - Requester Pays (요청자 지불)

![as14](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as14.png)

- 일반적으로는 버킷 소유자가 버킷과 관련된 모든 Amazon S3 스토리지 및 데이터 전송 비용을 지불
- 예시 1
	- 버킷 셋이 있고 그 안에 객체를 보관하고 있음
	- 그리고 요청자, 즉 사용자가 버킷으로부터 파일을 다운로드
	- 그러면 네트워킹 비용 역시 버킷 및 객체 소유자에게 청구됨
	- 그러나 수많은 대형 파일이 있고 일부 고객이 이를 다운로드하려고 하면 여러분은 요청자 지불 버킷을 활성화 해야 할 것
		- 이 경우 버킷 소유자가 아니라 요청자가 객체 데이터 다운로드 비용을 지불
- 예시 2
	- 소유자가 여전히 버킷의 객체 스토리지 비용을 부담하겠지만 요청자가 객체를 다운로드하면 이제 그 요청자가 다운로드와 관련된 네트워킹 비용을 지불
	- 대량의 데이터 셋을 다른 계정과 공유하려고 할 때 매우 유용
	- 그렇게 하려면 요청자가 익명이어서는 안 됨
		- 요청자가 AWS에서 인증을 받아야 함
		- AWS에서 인증을 받아야 AWS가 객체에 대한 특정 다운로드를 요청한 요청자에게 청구할 수 있기 때문

## Amazon S3 - Event Notifications

![as15](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as15.png)

- S3 이벤트 알림
- Amazon S3에서 이벤트가 발생
- 이벤트
	- 객체가 생성된 경우
	- 객체가 삭제되거나 복원된 경우
	- 복제가 발생한 경우
- 이벤트를 필터링할 수 있음
	- 예를 들어 .jpg로 끝나는 객체를 필터링할 수 있음
- 이벤트 알림은 Amazon S3에서 발생하는 특정 이벤트에 자동으로 반응하게 할 수 있음
- 예시
	- Amazon S3에 업로드되는 사진의 섬네일을 생성하려고 한다면 이에 대한 이벤트 알림을 만들어서 몇 가지 수신지로 보낼 수 있음
- **SNS 주제**가 될 수도 있고 **SQS 대기열**이나 **Lambda 함수**, **EventBridge**로 알람을 보냄
- 원하는 만큼 S3 이벤트를 생성하고 원하는 대상으로 보낼 수 있음
- 이벤트가 대상에 전달되는 시간은 일반적으로 몇 초밖에 안 걸리지만 가끔 1분 이상 걸릴 수도 있음

### S3 Event Notifications with Amazon EventBridge

![as16](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as16.png)

- 이벤트 알림의 새로운 기능으로 Amazon EventBridge와 통합됨
- 이벤트가 Amazon S3 버킷으로 이동하면 이벤트 종류와 상관없이 모든 이벤트는 Amazon EventBridge로 모이게 됨
- EventBridge에 규칙을 설정할 수 있고 설정한 규칙을 통해 18개가 넘는 AWS 서비스에 이벤트 알림을 보낼 수 있음
	- S3 이벤트 알림 기능을 크게 향상시켜 줌
- **고급 필터링 옵션**을 이전보다 훨씬 더 많이 사용할 수 있음
	- 메타데이터, 객체 크기 그리고 이름 등으로 필터링할 수 있고 동시에 여러 수신지에 보낼 수 있음
- **다양한 목적지**
	- 단계 함수나 Kinesis Streams 혹은 Firehose에도 보낼 수 있고 Amazon EventBridge에서 제공하는 기능을 사용할 수도 있음
- **Event Capability**
	- 이벤트를 보관(Archive)하거나 재생할 수 있고 보다 안정적으로 전송할 수 있음


## Amazon S3 Baseline Performance (기준 성능)

- 기본적으로 Amazon S3는 요청이 아주 많을 때 자동으로 확장됨
- S3로부터 첫 번째 바이트를 수신하는 데 지연 시간도 100~200밀리초 사이로 아주 짧음
- S3는 버킷 내에서 접두사당 초당 3,500개의 PUT/COPY/POST/DELETE 초당 5,500개의 GET/HEAD 요청을 지원
- '접두사당'', '초당'의 의미
	- 버킷 내에서 접두사 수는 제한이 없음
	- 예시
		- file이라는 이름을 가진 네 개의 객체가 있다고 가정
		- 첫 번째 객체의 위치는 bucket/folder1/sub1/file
		- bucket과 file 사이에 있는 것이 접두사
			- /folder1/sub1
			- 이 접두사를 갖는 file은 초당 3,500개의 PUT 요청과 초당 5,500개의 GET 요청을 처리
		- folder1에 sub2 폴더가 있을 경우 bucket과 file 사이에 있는 /folder1/sub2가 접두사
			- 이 접두사에 대해 3,500개의 PUT 요청과 5,500개의 GET 요청을 처리
		- 버킷 내에 1과 2 폴더가 있을 때는 접두사가 달라짐
		- 위와 같이 네 개의 접두사에 읽기 요청을 균등하게 분산하면 초당 22,000개의 GET/HEAD 요청을 처리할 수 있음

### S3 Performance

#### S3 Performance - Multi-Part Upload

![as17](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as17.png)

- 100MB가 넘는 파일은 멀티파트 업로드를 사용하는 것이 좋고 5GB가 넘는 파일은 반드시 사용해야 함
- 멀티파트 업로드는 업로드를 병렬화하므로 전송 속도를 높여 대역폭을 최대화할 수 있음
- 예시 (중요함)
	 - 큰 파일을 Amazon S3에 업로드하려고 함
	 - 파일을 여러 파트, 작은 청크로 나눔
	 - 각 파일은 Amazon S3에 병렬로 업로드됨
	 - Amazon S3에 모든 파트가 업로드되면 자동으로 모든 파트를 합쳐 다시 하나의 큰 파일로 만듬

#### S3 Performance - Transfer Acceleration

![as18](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as18.png)

- 업로드와 다운로드를 목적
- 퍼블릭 인터넷의 사용량을 최소화하고 프라이빗 AWS 네트워크의 사용량을 최대화하는 것
- 파일을 AWS 엣지 로케이션으로 전송해서 전송 속도를 높이고 데이터를 대상 리전에 있는 S3 버킷으로 전달
- 엣지 로케이션은 리전보다 수가 많음
	- 현재 엣지 로케이션은 200개가 넘고 그 수는 계속 증가하고 있음
- 전송 가속화는 멀티파트 업로드와 같이 사용할 수 있음
- 예시
	- 미국에 있는 파일을 호주에 있는 S3 버킷에 업로드하려고 함
	- 미국에 있는 엣지 로케이션을 통해 파일을 업로드하면 아주 빠를 것임
		- 퍼블릭 인터넷을 사용할 것임
	- 그런 다음 엣지 로케이션에서 호주에 있는 Amazon S3 버킷으로 빠른 프라이빗 AWS 네트워크를 통해 데이터를 전송

#### S3 Performance - Byte-Range Fetches

![as19](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as19.png)

- 파일을 수신하고 파일을 읽는 가장 효율적인 방법
- 'S3 바이트 범위 가져오기'라는 기능은 파일에서 특정 바이트 범위를 가져와서 GET 요청을 병렬화
- 특정 바이트 범위를 가져오는 데 실패한 경우에도 더 작은 바이트 범위에서 재시도하므로 실패의 경우에도 복원력이 높음
- 사용 사례: 다운로드 속도를 높일 때
- 예시 1
	- S3에 큰 파일이 있다고 가정
	- 첫 번째 파트를 요청
		- 파일 앞 부분의 몇 바이트
	- 두 번째 파트부터 N 번째 파트까지 요청해 모든 파트 부분을 특정 바이트 범위 가져오기로 요청
		- 파일의 특정 범위만 요청하므로 바이트 범위라고 함
	- 또한 모든 요청은 병렬화됨
		- GET 요청을 병렬화해서 다운로드 속도를 높이는 것
- 예시 2
	- 파일의 일부만 검색하는 것
	- S3에 있는 파일의 첫 50바이트가 헤더라는 것을 안다면 첫 50바이트를 사용해서 헤더에 대한 바이트 범위 요청만을 보내면 됨
	- 그럼 해당 정보를 빠르게 수신할 수 있음

## Amazon S3 - Select & Glacier Select

![as20](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as20.png)

- S3에서 파일을 검색할 때 검색한 다음 필터링하면 너무 많은 데이터를 검색하게 됨
- 대신 서버 측 필터링을 수행하면 SQL 문에서 간단히 행과 열을 사용해 필터링할 수 있음
- 네트워크 전송이 줄어들기 때문에 데이터 검색과 필터링에 드는 클라이언트 측의 CPU 비용도 줄어듬
- 예시 1
	- S3 Select 이전에는 모든 데이터를 검색한 다음 애플리케이션 측에서 필터링을 통해 필요한 것을 찾았음
	- 많은 데이터를 가져와서 약간의 데이터만 사용
	- 하지만 S3 Select를 사용하면 Amazon S3가 대신 파일을 필터링해 주고 필요한 데이터만 검색할 수 있음
	- 속도는 400% 빨라지고 비용은 80% 줄어듬
- 예시 2
	- Amazon S3 Select로 CSV 파일을 가져오기
	- Amazon S3는 이 CSV 파일을 찾아서 자체 서비스인 서버 측에서 필터링해서 우리에게 필터링된 데이터 세트를 보냄
		- 데이터 크기가 훨씬 작아 저렴해짐
- 간단한 필터링에는 S3 Select나 Glacier Select를 추천

## Amazon S3 Batch Operations

![as21](https://github.com/seungwonbased/TIL/blob/main/AWS/assets/as21.png)

- S3 Batch Operations는 단일 요청으로 기존 S3 객체에서 대량 작업을 수행하는 서비스
- Use Cases
	- 한 번에 많은 S3 객체의 메타데이터와 프로퍼티를 수정할 수 있고 배치 작업으로 S3 버킷 간에 객체를 복사할 수 있음
	- **S3 버킷 내 암호화되지 않은 모든 객체를 암호화할 수 있음**
	- ACL이나 태그를 수정할 수 있음
	- S3 Glacier에서 한 번에 많은 객체를 복원할 수 있음
	- Lambda 함수를 호출해 S3 Batch Operations의 모든 객체에서 사용자 지정 작업을 수행할 수도 있음
	- 객체 목록에서 원하는 작업은 무엇이든지 수행할 수 있음
		- 작업은 객체의 목록, 수행할 작업 옵션 매개 변수로 구성
- S3 Batch Operations를 사용하면 
	- 재시도를 관리할 수 있음
	- 진행 상황을 추적할 수 있음
	- 작업 완료 알림을 보낼 수 있음
	- 보고서 생성 등을 할 수 있음
- S3 배치에 전달할 객체 목록은 S3 Inventory라는 기능을 사용해 객체 목록을 가져오고 S3 Select를 사용해 객체를 필터링
	- S3 Inventory와 S3 Select를 사용해서 배치 작업에 포함하려는 필터링된 객체 목록을 얻은 다음 S3 Batch Operations에 수행할 작업, 매개 변수와 함께 객체 목록을 전달
	- 그러면 S3 배치가 작업을 수행하고 객체를 처리
- 알아둬야 할 주요 사용 사례는 S3 Inventory를 사용해 암호화되지 않은 모든 객체를 찾은 다음 
S3 Batch Operations를 사용해 한 번에 모두 암호화하는 것